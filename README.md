# 리코 사전과제 02 보고서

## 1. 환경 및 의존성

* **Python**: 3.7 이상 버전
* **Browser**: Google Chrome (최신 버전 권장) 및 대응되는 ChromeDriver
* **Libraries**:
* `selenium`: 웹 자동화 및 동적 요소 제어
* `pandas`: 수집 데이터 구조화 및 파일 저장
* `re`, `time`, `datetime`: 데이터 정제 및 시간 제어 (내장 라이브러리)

---

## 2. 로컬 실행 방법

본 프로젝트는 쉘 스크립트를 통해 환경 확인 및 실행을 자동화하고 있습니다.

### 1) 실행 권한 부여

```bash
chmod +x run.sh

```

### 2) 스크립트 실행

```bash
./run.sh

```

---

## 3. 설계 및 주요 가정

### 1) 컴포넌트 기반 모듈 설계

유지보수와 재사용성을 극대화하기 위해 각 역할을 독립적인 모듈로 분리하였습니다.

* **PageValidator**: 로딩바 감지 및 비동기 팝업 제거 등 환경 정화 담당
* **PageMover**: 메뉴 이동 및 페이지네이션 등 내비게이션 담당
* **OptionSearcher**: 검색어 주입 및 조회 버튼 실행 등 인터페이스 제어 담당
* **DataCrawler**: DOM 탐색, 인덱스 기반 수집 및 데이터 정제 담당
* **DataHandler**: 수집 데이터의 다중 포맷(CSV, JSON) 저장 및 파이프라인 담당
* **DataValidator***: 수집 데이터의 중복 여부 판단 담당

  
<img width="657" height="506" alt="스크린샷 2026-02-09 오후 10 38 12" src="https://github.com/user-attachments/assets/1c024258-c64e-4c16-88f9-184e6cbb121b" />



### 2) 모듈 분리 설계 근거

* **관심사 분리(SoC)**: 웹 구조 변경 시 전체 코드가 아닌 해당 도메인 모듈만 수정하여 사이드 이펙트를 최소화합니다.
* **재사용성**: `Validator`나 `Handler` 모듈은 인터페이스가 규격화되어 있어 타 사이트 크롤링 엔진 구축 시 별도의 수정 없이 즉시 사용이 가능합니다.

작성하신 내용은 프로젝트의 핵심 성과를 잘 담고 있지만, **'제품 수준(Product Level)'**을 강조하는 기술 과제인 만큼 용어를 더 전문적으로 다듬고, 우리가 최종적으로 개선한 **'동적 대기(Explicit Wait)'**와 **'체크포인트(Checkpoint)'** 전략을 강조하는 것이 좋습니다.

특히 `time.sleep(5)` 사용은 "안정적"이라고 표현하기보다 "초기 모델의 한계를 극복하기 위한 과도기적 설정"이었음을 명시하고, 현재는 **'이벤트 기반 대기'**로 최적화했음을 보여주는 것이 훨씬 유리합니다.

---

### 3) 주요 로직에 따른 제품 관점 최적화

크롤링 파이프라인의 각 단계별 병목 구간을 분석하고, 서비스 지속 가능성을 고려한 최적화를 진행했습니다.

**[1] 웹 사이트 접속 및 환경 검증**

* **최적화**: 비동기 로딩 환경에서 로딩바(`Spinner`)의 출현 여부와 관계없이 페이지 안정화를 보장하기 위해 `WebDriverWait`를 이용한 **명시적 대기(Explicit Wait)**를 15초로 설정했습니다. 단순 시간 대기보다 자원 낭비를 줄이고 실행 속도를 극대화했습니다.

**[2] 지능형 팝업 제어**

* **최적화**: 초기 E2E 테스트 결과를 반영하여 5초의 안전 대기 시간을 설정했으나 최종적으로는 **다중 반복 감지(Iterative Detection)** 로직을 적용했습니다. 팝업의 물리적 출력 여부를 실시간 검증하여 팝업이 제거된 즉시 다음 로직으로 진입하도록 개선했습니다.

**[3] 내비게이션 추상화 (입찰공고 외 확장)**

* **최적화**: 특정 카테고리에 종속되지 않도록 메뉴 접근 로직을 **파라미터화(Parameterized)**했습니다. `main.py`에서 전달하는 키워드에 따라 동적으로 경로를 생성하므로 향후 '입찰결과', '계약현황' 등 타 서비스 확장 시 코드 수정 없이 대응 가능하게 하였습니다.

**[4] 동적 필터링 시스템**

* **최적화**: 고정된 검색 조건이 아닌 사용자 요구에 따른 가변적 필터링이 가능하도록 검색 모듈을 재구성했습니다. 이는 서비스의 유연성을 높여 다양한 비즈니스 케이스에 대응할 수 있는 기반을 마련했습니다.

**[5] 데이터 수집 가속화**

* **최적화**: 수집 단계에서 발생하던 불필요한 유휴 시간(`time.sleep`)을 제거하고 DOM 요소의 가시성(Visibility)을 기준으로 작동하는 **이벤트 기반 대기 전략**을 도입하여 데이터 수집 속도를 획기적으로 향상시켰습니다.

**[6] 다중 포맷 파이프라인 (CSV/JSON)**

* **최적화**: `DataHandler` 모듈을 통해 데이터 출력 형식을 인터페이스화했습니다. CSV뿐만 아니라 JSON, Excel 등 다양한 데이터 포맷을 지원하여 타 시스템과의 데이터 연동성을 확보했습니다.

---

### 4) 안정적인 동적 크롤링 코드 작성 전략

**[1] 체크포인트(Checkpoint) 기반 재시도 전략**

* **구현**: 네트워크 단절이나 시스템 다운 등 예외 상황 발생 시, 기수집된 데이터의 **고유 고유번호(ID)를 실시간으로 파일에 기록**합니다. 재실행 시 해당 체크포인트를 참조하여 중복을 건너뛰고 멈춘 지점부터 수집을 재개하는 **회복 탄력성(Resilience)**을 확보했습니다.

**[2] 요소 제어 무결성 및 중복 방지**

* **구현**: 동일한 기능을 수행하는 중복 버튼이나 레이어 간섭 문제를 해결하기 위해 **JavaScript Executor**를 활용한 직접 호출 방식을 채택했습니다. 이는 물리적 마우스 이벤트보다 정확도가 높으며 동적 UI 환경에서 안정적인 제어를 보장합니다.

**[3] 텍스트 정규화 및 키워드 정제**

* **구현**: 웹 페이지에 산재한 제어 문자(줄바꿈, 탭, 연속 공백)를 제거하기 위해 **정규표현식(Regex)** 기반의 클리닝 파이프라인을 구축했습니다. 이를 통해 별도의 가공 없이 즉시 비즈니스 분석에 활용 가능한 수준의 고품질 데이터를 산출합니다.


### 5) 제품 수준의 예외 처리 및 복구 로직

* **Stale Element 회복 탄력성**: 상세 페이지 진입 후 목록으로 복귀할 때 DOM이 재렌더링되며 기존 요소 객체가 소멸하는 문제를 확인했습니다. 이를 해결하기 위해 매 루프마다 요소 리스트를 최신화(Re-fetching)하는 구조를 구현하여 수집의 연속성을 확보했습니다.
* **확장성 있는 로케이터 전략**: 버튼 클릭 시 특정 ID에만 의존하지 않고, 텍스트 키워드 매칭과 속성 값을 조합한 복합 로케이터를 사용하여 사이트 UI 업데이트에 유연하게 대응하도록 설계했습니다.
* **데이터 정제 자동화**: 수집 단계에서 정규표현식을 활용해 불필요한 제어 문자(줄바꿈, 탭 등)를 제거하는 전처리 로직을 내장하여 바로 분석에 활용 가능한 수준의 데이터를 산출합니다.

### 6) 안정적인 수집을 위한 추가 최적화

* **컨텍스트 기반 팝업 제어**: 단순히 페이지 로딩 직후에만 팝업을 닫는 것이 아니라, 상세 페이지 진입 전후로 발생할 수 있는 비동기 팝업을 감지하기 위해 **다중 반복 감지(Iterative Detection)** 로직을 적용했습니다.
* **내결함성(Fault Tolerance) 설계**: 특정 행(Row) 수집 중 예외가 발생하더라도 전체 프로세스가 중단되지 않도록 `Try-Except` 블록을 세분화하여 설계했습니다. 실패한 항목은 로그에 기록하고 즉시 다음 행으로 넘어감으로써 대량 수집 시의 안정성을 보장했습니다.
* **리소스 최적화**: 자바스크립트 실행(`execute_script`)을 통한 요소 제어를 우선순위에 두어, 물리적 마우스 이벤트 발생 시 생길 수 있는 브라우저 포커스 문제나 오작동 가능성을 원천적으로 차단했습니다.

---

## 4. 한계 및 개선 아이디어

제공해주신 블로그 포스팅의 **무인 운영 자동화, 성능 최적화(병렬 처리, 캐싱), 지능형 데이터 추출** 기법들을 참고하여, `README.md`의 **[4. 한계 및 개선 아이디어]** 섹션을 전문가 수준으로 보강해 드립니다.

---

## 4. 한계 및 개선 아이디어

### 1) 현재의 한계점

* **단일 프로세스의 성능 병목**: 현재 순차적(Sequential) 처리 방식으로 인해 서울 전역과 같은 대규모 지역 수집 시 시간 복잡도가 선형적으로 증가하는 한계가 있습니다.
* **데이터 휘발성 및 중복 요청**: 동일한 페이지에 대한 반복 요청을 제어하는 캐싱 메커니즘이 없어 네트워크 자원 낭비와 IP 차단 리스크가 존재합니다.
* **동적 렌더링 데이터 누락**: BeautifulSoup 기반의 정적 파싱만으로는 JavaScript 전역 변수에 숨겨진 좌표 정보나 '더보기' 클릭 후 나타나는 영업시간 데이터를 완벽히 수집하기 어렵습니다.
* **운영 모니터링 부재**: 시스템 다운, IP 차단, 웹사이트 구조 변경 등 돌발 상황 발생 시 개발자가 실시간으로 인지하고 대응할 수 있는 알림 체계가 미흡합니다.

### 2) 향후 개선 아이디어 (Roadmap)

#### ** 성능 극대화 (Efficiency)**

* **Multi-processing 기반 병렬 수집**: `multiprocessing` 모듈의 `ProcessPoolExecutor`를 도입하여 지역별/키워드별로 작업을 분할하고, CPU 코어 수에 최적화된 병렬 처리를 통해 수집 시간을 최대 30배 이상 단축합니다.
* **Redis 기반 다층 캐싱 전략**: 인메모리 데이터 저장소인 Redis를 도입하여 이미 방문한 URL이나 검색 결과(가게 목록)를 캐싱함으로써 불필요한 네트워크 요청을 50% 이상 차단합니다.
* **Bulk Insert 최적화**: 개별 `INSERT` 대신 PostgreSQL의 `COPY` 명령어 또는 Bulk 로직을 사용하여 DB 적재 속도를 20배 이상 향상시킵니다.

#### ** 지능형 수집 및 데이터 품질 (Intelligence)**

* **JavaScript 변수 직접 추출**: Selenium의 `execute_script`를 활용하여 HTML 소스에는 드러나지 않는 `window.PLACE_INFO` 등의 전역 변수에서 좌표 데이터를 직접 추출하여 정확도를 높입니다.
* **ML 기반 중복 제거 및 유효성 검증**: TF-IDF 텍스트 유사도 분석과 위치 기반 클러스터링(DBSCAN)을 결합하여 중복 데이터를 자동 탐지하고, 지오코딩 재시도를 통해 잘못된 위치 정보를 자동 교정합니다.
* **사용자 행동 시뮬레이션**: '영업시간 더보기'와 같은 동적 요소에 대해 자동 클릭 및 렌더링 대기 로직을 강화하여 숨겨진 상세 정보를 완벽하게 수집합니다.

#### ** 무인 운영 자동화 (Automation & Reliability)**

* **스스로 치유하는 시스템 (Self-Healing)**: IP 차단 감지 시 Proxy IP 및 User-Agent 자동 교체, 3회 이상 실패 시 서비스 자동 재시작 등 무인 운영을 위한 복구 메커니즘을 구축합니다.
* **실시간 모니터링 및 스마트 리포팅**: Slack/Discord API를 연동하여 일일 수집 통계 및 품질 점수를 보고하고, 시스템 장애 시 즉시 긴급 알림을 발송하는 체계를 구축합니다.
* **가게 상태 자동 업데이트**: 최신 리뷰 유무, 웹사이트 접속 여부 등을 주기적으로 체크하여 폐업 의심 가게를 자동으로 분류하고 데이터의 최신성을 유지합니다.
