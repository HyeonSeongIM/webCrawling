# 리코 사전과제 02 보고서

## 1. 환경 및 의존성

* **Python**: 3.7 이상 버전
* **Browser**: Google Chrome (최신 버전 권장) 및 대응되는 ChromeDriver
* **Libraries**:
* `selenium`: 웹 자동화 및 동적 요소 제어
* `pandas`: 수집 데이터 구조화 및 파일 저장
* `re`, `time`, `datetime`: 데이터 정제 및 시간 제어 (내장 라이브러리)

---

## 2. 로컬 실행 방법

본 프로젝트는 쉘 스크립트를 통해 환경 확인 및 실행을 자동화하고 있습니다.

### 1) 실행 권한 부여

```bash
chmod +x run.sh

```

### 2) 스크립트 실행

```bash
./run.sh

```

---

## 3. 설계 및 주요 가정

### 1) 컴포넌트 기반 모듈 설계

유지보수와 재사용성을 극대화하기 위해 각 역할을 독립적인 모듈로 분리하였습니다.

* **PageValidator**: 로딩바 감지 및 비동기 팝업 제거 등 환경 정화 담당
* **PageMover**: 메뉴 이동 및 페이지네이션 등 내비게이션 담당
* **OptionSearcher**: 검색어 주입 및 조회 버튼 실행 등 인터페이스 제어 담당
* **DataCrawler**: DOM 탐색, 인덱스 기반 수집 및 데이터 정제 담당
* **DataHandler**: 수집 데이터의 다중 포맷(CSV, JSON) 저장 및 파이프라인 담당
* **DataValidator***: 수집 데이터의 중복 여부 판단 담당

  
<img width="657" height="506" alt="스크린샷 2026-02-09 오후 10 38 12" src="https://github.com/user-attachments/assets/1c024258-c64e-4c16-88f9-184e6cbb121b" />



### 2) 모듈 분리 설계 근거

* **관심사 분리(SoC)**: 웹 구조 변경 시 전체 코드가 아닌 해당 도메인 모듈만 수정하여 사이드 이펙트를 최소화합니다.
* **재사용성**: `Validator`나 `Handler` 모듈은 인터페이스가 규격화되어 있어 타 사이트 크롤링 엔진 구축 시 별도의 수정 없이 즉시 사용이 가능합니다.

작성하신 내용은 프로젝트의 핵심 성과를 잘 담고 있지만, **'제품 수준(Product Level)'**을 강조하는 기술 과제인 만큼 용어를 더 전문적으로 다듬고, 우리가 최종적으로 개선한 **'동적 대기(Explicit Wait)'**와 **'체크포인트(Checkpoint)'** 전략을 강조하는 것이 좋습니다.

특히 `time.sleep(5)` 사용은 "안정적"이라고 표현하기보다 "초기 모델의 한계를 극복하기 위한 과도기적 설정"이었음을 명시하고, 현재는 **'이벤트 기반 대기'**로 최적화했음을 보여주는 것이 훨씬 유리합니다.

---

### 3) 주요 로직에 따른 제품 관점 최적화

크롤링 파이프라인의 각 단계별 병목 구간을 분석하고, 서비스 지속 가능성을 고려한 최적화를 진행했습니다.

**[1] 웹 사이트 접속 및 환경 검증**

* **최적화**: 비동기 로딩 환경에서 로딩바(`Spinner`)의 출현 여부와 관계없이 페이지 안정화를 보장하기 위해 `WebDriverWait`를 이용한 **명시적 대기(Explicit Wait)**를 15초로 설정했습니다. 단순 시간 대기보다 자원 낭비를 줄이고 실행 속도를 극대화했습니다.

**[2] 지능형 팝업 제어**

* **최적화**: 초기 E2E 테스트 결과를 반영하여 5초의 안전 대기 시간을 설정했으나 최종적으로는 **다중 반복 감지(Iterative Detection)** 로직을 적용했습니다. 팝업의 물리적 출력 여부를 실시간 검증하여 팝업이 제거된 즉시 다음 로직으로 진입하도록 개선했습니다.

**[3] 내비게이션 추상화 (입찰공고 외 확장)**

* **최적화**: 특정 카테고리에 종속되지 않도록 메뉴 접근 로직을 **파라미터화(Parameterized)**했습니다. `main.py`에서 전달하는 키워드에 따라 동적으로 경로를 생성하므로 향후 '입찰결과', '계약현황' 등 타 서비스 확장 시 코드 수정 없이 대응 가능하게 하였습니다.

**[4] 동적 필터링 시스템**

* **최적화**: 고정된 검색 조건이 아닌 사용자 요구에 따른 가변적 필터링이 가능하도록 검색 모듈을 재구성했습니다. 이는 서비스의 유연성을 높여 다양한 비즈니스 케이스에 대응할 수 있는 기반을 마련했습니다.

**[5] 데이터 수집 가속화**

* **최적화**: 수집 단계에서 발생하던 불필요한 유휴 시간(`time.sleep`)을 제거하고 DOM 요소의 가시성(Visibility)을 기준으로 작동하는 **이벤트 기반 대기 전략**을 도입하여 데이터 수집 속도를 획기적으로 향상시켰습니다.

**[6] 다중 포맷 파이프라인 (CSV/JSON)**

* **최적화**: `DataHandler` 모듈을 통해 데이터 출력 형식을 인터페이스화했습니다. CSV뿐만 아니라 JSON, Excel 등 다양한 데이터 포맷을 지원하여 타 시스템과의 데이터 연동성을 확보했습니다.

---

### 4) 안정적인 동적 크롤링 코드 작성 전략

**[1] 체크포인트(Checkpoint) 기반 재시도 전략**

* **구현**: 네트워크 단절이나 시스템 다운 등 예외 상황 발생 시, 기수집된 데이터의 **고유 고유번호(ID)를 실시간으로 파일에 기록**합니다. 재실행 시 해당 체크포인트를 참조하여 중복을 건너뛰고 멈춘 지점부터 수집을 재개하는 **회복 탄력성(Resilience)**을 확보했습니다.

**[2] 요소 제어 무결성 및 중복 방지**

* **구현**: 동일한 기능을 수행하는 중복 버튼이나 레이어 간섭 문제를 해결하기 위해 **JavaScript Executor**를 활용한 직접 호출 방식을 채택했습니다. 이는 물리적 마우스 이벤트보다 정확도가 높으며 동적 UI 환경에서 안정적인 제어를 보장합니다.

**[3] 텍스트 정규화 및 키워드 정제**

* **구현**: 웹 페이지에 산재한 제어 문자(줄바꿈, 탭, 연속 공백)를 제거하기 위해 **정규표현식(Regex)** 기반의 클리닝 파이프라인을 구축했습니다. 이를 통해 별도의 가공 없이 즉시 비즈니스 분석에 활용 가능한 수준의 고품질 데이터를 산출합니다.


### 5) 제품 수준의 예외 처리 및 복구 로직

* **Stale Element 회복 탄력성**: 상세 페이지 진입 후 목록으로 복귀할 때 DOM이 재렌더링되며 기존 요소 객체가 소멸하는 문제를 확인했습니다. 이를 해결하기 위해 매 루프마다 요소 리스트를 최신화(Re-fetching)하는 구조를 구현하여 수집의 연속성을 확보했습니다.
* **확장성 있는 로케이터 전략**: 버튼 클릭 시 특정 ID에만 의존하지 않고, 텍스트 키워드 매칭과 속성 값을 조합한 복합 로케이터를 사용하여 사이트 UI 업데이트에 유연하게 대응하도록 설계했습니다.
* **데이터 정제 자동화**: 수집 단계에서 정규표현식을 활용해 불필요한 제어 문자(줄바꿈, 탭 등)를 제거하는 전처리 로직을 내장하여 바로 분석에 활용 가능한 수준의 데이터를 산출합니다.

### 6) 안정적인 수집을 위한 추가 최적화

* **컨텍스트 기반 팝업 제어**: 단순히 페이지 로딩 직후에만 팝업을 닫는 것이 아니라, 상세 페이지 진입 전후로 발생할 수 있는 비동기 팝업을 감지하기 위해 **다중 반복 감지(Iterative Detection)** 로직을 적용했습니다.
* **내결함성(Fault Tolerance) 설계**: 특정 행(Row) 수집 중 예외가 발생하더라도 전체 프로세스가 중단되지 않도록 `Try-Except` 블록을 세분화하여 설계했습니다. 실패한 항목은 로그에 기록하고 즉시 다음 행으로 넘어감으로써 대량 수집 시의 안정성을 보장했습니다.
* **리소스 최적화**: 자바스크립트 실행(`execute_script`)을 통한 요소 제어를 우선순위에 두어, 물리적 마우스 이벤트 발생 시 생길 수 있는 브라우저 포커스 문제나 오작동 가능성을 원천적으로 차단했습니다.

---

## 4. 한계 및 개선 아이디어

### 1) 현재의 한계점

* **단일 스레드 구조**: 현재 동기식 방식으로 작동하여 대량의 페이지를 수집할 때 속도 측면의 한계가 있습니다.
* **브라우저 의존성**: Chrome 환경에 최적화되어 있어 타 브라우저(Firefox, Edge) 대응 로직이 부족합니다.
* **자동 캡차(CAPTCHA) 대응**: 사이트 보안 정책에 따른 캡차 발생 시 수동 개입이 필요할 수 있습니다.

### 2) 향후 개선 아이디어

* **멀티 프로세싱 적용**: `multiprocessing` 또는 `Selenium Grid`를 도입하여 여러 페이지를 동시 수집함으로써 성능을 획기적으로 개선할 수 있습니다.
* **Headless 모드 및 리소스 최적화**: GUI 없는 모드로 실행하고 불필요한 이미지 로딩을 차단하여 서버 리소스 점유율을 낮출 수 있습니다.
* **에러 알림 시스템**: 수집 중 치명적 오류 발생 시 Slack 또는 이메일로 즉시 알림을 보내는 모니터링 기능을 추가할 수 있습니다.
* **로그 추적 강화**: 단순 터미널 출력을 넘어 `logging` 모듈을 통해 에러 시점의 스크린샷과 스택 트레이스를 기록하는 로깅 시스템 고도화가 가능합니다.
